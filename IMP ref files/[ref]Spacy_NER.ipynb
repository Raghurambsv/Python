{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/natural-language-processing-spacy-python/\n",
    "\n",
    "# https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n",
    "\n",
    "# https://www.youtube.com/watch?v=08NbfA9od9w&list=PLc2rvfiptPSQgsORc7iuv7UxhbRJox-pW&index=8\n",
    "\n",
    "# https://github.com/prem2017/new-entity-labelling/blob/master/new_entity_labelling.ipynb\n",
    "\n",
    "# https://github.com/explosion/projects/blob/master/nel-emerson/scripts/notebook_video.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "complete_text = ('Gus Proto is a Python developer currently'\n",
    "     'working for a London-based Fintech company. He is'\n",
    "     ' interested in learning Natural Language Processing.'\n",
    "     ' There is a developer conference happening on 21 July'\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "     ' Language Processing\". There is a helpline number '\n",
    "     ' available at +1-1234567891. Gus is helping organize it.'\n",
    "    \n",
    "                )\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    " # Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "          if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    " # 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)\n",
    "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n",
    " # Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)\n",
    "\n",
    "\n",
    "#Lemma\n",
    "for token in complete_doc[:10]:\n",
    "    print (token, token.lemma_)\n",
    "    \n",
    "#STOP WORDS\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('No of stop words:',len(spacy_stopwords))\n",
    "about_no_stopword_doc = [token for token in complete_doc if not token.is_stop]\n",
    "print (about_no_stopword_doc)\n",
    "\n",
    "#Preprocessing\n",
    "def is_token_allowed(token):\n",
    "    '''Only allow valid tokens which are not stop words and punctuation symbols.'''\n",
    "    if (not token or not token.string.strip() or token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "# Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = [preprocess_token(token)for token in complete_doc if is_token_allowed(token)]\n",
    "complete_filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FYI: use NER in spacy before STEMMING & making it lowercase \n",
    "#### (coz \"Apple\" is company.... \"apple\" is not entity only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "texts=['net income was $9.4 million comapred to last year 3.4$ million',\n",
    "      'revenue exceeds twelve billion dollars will loss of $1b']\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "docs=nlp.pipe(texts,disable=['tagger','parser'])\n",
    "\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List format & NER Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "list_of_text_data=texts=['net income was $9.4 million comapred to last year 3.4$ million',\n",
    "      'revenue exceeds twelve billion dollars will loss of $1b','Google is very great office']\n",
    "docs = [nlp(text) for text in list_of_text_data]\n",
    "displacy.render(docs,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a doc on news articles\n",
    "news_text=\"\"\"Indian man has allegedly duped nearly 50 businessmen in the UAE of USD 1.6 million and fled the country in the most unlikely way -- on a repatriation flight to Hyderabad, according to a media report on Saturday.Yogesh Ashok Yariava, the prime accused in the fraud, flew from Abu Dhabi to Hyderabad on a Vande Bharat repatriation flight on May 11 with around 170 evacuees, the Gulf News reported.Yariava, the 36-year-old owner of the fraudulent Royal Luck Foodstuff Trading, made bulk purchases worth 6 million dirhams (USD 1.6 million) against post-dated cheques from unsuspecting traders before fleeing to India, the daily said.\n",
    "The bought goods included facemasks, hand sanitisers, medical gloves (worth nearly 5,00,000 dirhams), rice and nuts (3,93,000 dirhams), tuna, pistachios and saffron (3,00,725 dirhams), French fries and mozzarella cheese (2,29,000 dirhams), frozen Indian beef (2,07,000 dirhams) and halwa and tahina (52,812 dirhams).\n",
    "The list of items and defrauded persons keeps getting longer as more and more victims come forward, the report said.\n",
    "The aggrieved traders have filed a case with the Bur Dubai police station.\n",
    "The traders said when the dud cheques started bouncing they rushed to the Royal Luck's office in Dubai but the shutters were down, even the fraudulent company's warehouses were empty.\"\"\"\n",
    "\n",
    "news_doc=nlp(news_text)\n",
    "\n",
    "# Function to identify  if tokens are named entities and replace them with UNKNOWN\n",
    "def remove_details(word):\n",
    "  if word.ent_type_ =='PERSON' or word.ent_type_=='ORG' or word.ent_type_=='GPE':\n",
    "    return ' UNKNOWN '\n",
    "  return word.string\n",
    "\n",
    "\n",
    "# Function where each token of spacy doc is passed through remove_deatils()\n",
    "def update_article(doc):\n",
    "  # Passing each token through remove_details() function.\n",
    "  tokens = map(remove_details,doc)\n",
    "  return ''.join(tokens)\n",
    "\n",
    "# Passing our news_doc to the function update_article()\n",
    "update_article(news_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# default sentence boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "docs=nlp(\"Welcome to ... spacy world... please tell us how you feel\")\n",
    "\n",
    "for doc in docs.sents:\n",
    "    print('sentence:',doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Function for sentence bounndary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#CUSTOM RULE FUNCTION (Split sentence when 3 dots encountered)\n",
    "def my_rule(doc):\n",
    "    for token in doc:\n",
    "        if token.text == '...':\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(my_rule,before='parser')\n",
    "#nlp.remove_pipe(my_rule)  ===> if you want to remove the custom rule\n",
    "\n",
    "docs=nlp(\"Welcome to ... spacy world... please tell us how you feel\")\n",
    "\n",
    "for idx,sent in enumerate(docs.sents):\n",
    "    print('sentence',idx,':',sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Function: include prefix of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from spacy.Matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp('Dr. Alex Smith chaired first board meeting at Google and 1 million')\n",
    "    \n",
    "print([(ent.text,ent.label_) for ent in doc.ents])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to include ('Dr','Dr.','Mr','Mr.') too\n",
    "def add_title(doc):\n",
    "    new_ents=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON' and ent.start!=0:\n",
    "            prev_token = doc[ent.start-1]\n",
    "            if prev_token.text in ('Dr','Dr.','Mr','Mr.'):\n",
    "                new_ent = Span(doc,ent.start-1,ent.end,label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "    doc.ents =new_ents\n",
    "    return doc\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(add_title,after='ner')\n",
    "doc=nlp('Dr. Alex Smith chaired first board meeting at Google and 1 million')\n",
    "print([(ent.text,ent.label_) for ent in doc.ents])    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EntityRuler : Add custom NER with exact pattern match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "text='Tony Stark owns the company StarkEnterprises  She '\n",
    "doc=nlp(text)\n",
    "\n",
    "# Printing the named entities\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_,ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "# Initialize\n",
    "nlp=spacy.load('en')\n",
    "ruler = EntityRuler(nlp)\n",
    "pattern=[{\"label\": \"RESEARCH FIELDS\", \"pattern\": \"statistics\"},{\"label\": \"RESEARCH FIELDS\", \"pattern\": \"maths\"}]\n",
    "ruler.add_patterns(pattern)\n",
    "nlp.add_pipe(ruler)\n",
    "text=\"\"\"I recently published my work fanfiction by Dr.X . \n",
    "        Tony Stark owns the company StarkEnterprises . \n",
    "        You should try My guide to statistics for clear concepts.\n",
    "        and also in field of maths \n",
    "     \"\"\"\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.gold import GoldParse \n",
    "# from spacy.language import EntityRecognizer \n",
    "from spacy.pipeline import EntityRecognizer\n",
    "\n",
    "nlp = spacy.load('en', entity = False, parser = False) \n",
    "\n",
    "doc_list = [] \n",
    "doc = nlp('Llamas make great pets.') \n",
    "doc_list.append(doc) \n",
    "gold_list = [] \n",
    "gold_list.append(GoldParse(doc, [u'ANIMAL', u'O', u'O', u'O'])) \n",
    "\n",
    "ner = EntityRecognizer(nlp.vocab, entity_types = ['ANIMAL']) \n",
    "ner.update(doc_list, gold_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "\n",
    "train_data = [\n",
    "    ('Who is Chaka Khan?', [(7, 17, 'PERSON')]),\n",
    "    ('I like London and Berlin.', [(7, 13, 'LOC'), (18, 24, 'LOC')])\n",
    "]\n",
    "\n",
    "# nlp = spacy.load('en', entity=False, parser=False)\n",
    "nlp = spacy.blank('en', entity=False, parser=False)\n",
    "\n",
    "ner = EntityRecognizer(nlp.vocab, entity_types=['PERSON', 'LOC'])\n",
    "\n",
    "for itn in range(5):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "\n",
    "        nlp.tagger(doc)\n",
    "        ner.update(doc, gold)\n",
    "ner.model.end_training()\n",
    "\n",
    "\n",
    "text = 'My name is Khan, Chaka Khan and I have been to London and Berlin'\n",
    "doc = nlp(text)\n",
    "print(doc.ents)\n",
    "for x in doc.ents:\n",
    "    if x.text:\n",
    "        print(x.text.strip(), x.label_.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint of complete program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.2.4\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning from \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----> train with additional examples of ORG & PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
    "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
    "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
    "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
    "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
    "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
    "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
    "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
    "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
    "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n",
    "\n",
    "doc=nlp(article_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting NER data from en_core_web_sm (Transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner=nlp.get_pipe(\"ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotated data for more examples of ORG,PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "        \n",
    "pipe_exceptions=[\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"]\n",
    "unaffected_pipes=[pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & update the NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the  model to directory\n",
    "output_dir = Path('/Users/c8907070/Desktop/NER')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NER from a blank SpaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "\n",
    "nlp.add_pipe(nlp.create_pipe('ner'))\n",
    "\n",
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]\n",
    "\n",
    "for _,annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "        \n",
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "# with nlp.disable_pipes(*unaffected_pipes): (This line not needed since its BLANK MODEL)\n",
    "\n",
    "# Training for 30 iterations\n",
    "for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)\n",
    "ner.model.end_training() \n",
    "# Save the  model to directory\n",
    "output_dir = Path('/Users/c8907070/Desktop/NER')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training completely NEW ENTITY type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"FOOD\" is new entity introduced and updated to existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.get_pipe('ner')\n",
    "\n",
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new Label & Resume training (Transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "# optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "move_names = list(ner.move_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "\n",
    "  sizes = compounding(1.0, 4.0, 1.001)\n",
    "  # Training for 30 iterations     \n",
    "  for itn in range(30):\n",
    "    # shuffle examples before training\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "    # ictionary to store losses\n",
    "    losses = {}\n",
    "    for batch in batches:\n",
    "      texts, annotations = zip(*batch)\n",
    "      # Calling update() over the iteration\n",
    "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "      print(\"Losses\", losses)\n",
    "        \n",
    "  ner.model.end_training()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Model\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the above output. The model has correctly identified the FOOD items. Also, notice that I had not passed ” Maggi ” as a training example to the model. Still, based on the similarity of context, the model has identified “Maggi” also asFOOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory\n",
    "from pathlib import Path\n",
    "output_dir=Path('/Users/c8907070/Desktop/NER/')\n",
    "\n",
    "# Saving the model to the output directory\n",
    "if not output_dir.exists():\n",
    "  output_dir.mkdir()\n",
    "nlp.meta['name'] = 'my_ner_food_added'  # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Loading the model from the directory\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp2('Dosa is an extremely famous south Indian dish')\n",
    "for ent in doc2.ents:\n",
    "  print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# #Add textcat\n",
    "nlp.add_pipe(nlp.create_pipe('textcat'),first=True)\n",
    "\n",
    "\n",
    "# #ADD Custom Function\n",
    "def CUST_FUNC1(doc):\n",
    "    length=len(doc)\n",
    "    print('CUSTOM FUNC :length of doc is',length)\n",
    "    return doc\n",
    "    \n",
    "# before,after,last=True,first=True    \n",
    "nlp.add_pipe(CUST_FUNC1, before='ner')\n",
    "\n",
    "\n",
    "# #Remove pipe commponent\n",
    "nlp.remove_pipe('textcat')\n",
    "\n",
    "\n",
    "# # RENAME pipe component\n",
    "nlp.rename_pipe(old_name='ner',new_name='my_custom_ner')\n",
    "\n",
    "\n",
    "# Call the nlp object on your text to activate the custom function\n",
    "doc = nlp(\" The Hindu Newspaper has increased the cost. I usually read the paper  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhraseMatcher & SPAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PhraseMatcher from spacy and intialize with a model's vocab\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# List of book names to be matched \n",
    "book_names = ['Pride and prejudice','Mansfield park','The Tale of Two cities','Great Expectations']\n",
    "\n",
    "# Creating pattern - list of docs through nlp.pipe() to save time\n",
    "book_patterns = list(nlp.pipe(book_names))\n",
    "\n",
    "# Adding the pattern to the matcher\n",
    "matcher.add(\"identify_books\", None, *book_patterns)\n",
    "\n",
    "# Import Span to slice the Doc\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Define the custom pipeline component\n",
    "def identify_books(doc):\n",
    "    # Apply the matcher to YOUR doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign them under label \"BOOKS\"\n",
    "    spans = [Span(doc, start, end, label=\"BOOKS\") for match_id, start, end in matches]\n",
    "    # Store the matched spans in doc.ents\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Adding the custom component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(identify_books, after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Calling the nlp object on the text\n",
    "doc = nlp(\"The library has got several new copies of Mansfield park and Great Expectations . I have filed a suggestion to buy more copies of The Tale of Two cities \")\n",
    "\n",
    "# Printing entities and their labels to verify\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "conference_org_text = ('There is a developer conference'\n",
    "     'happening on 21 July 2019 in London. It is titled'\n",
    "     ' \"Applications of Natural Language Processing\".'\n",
    "     ' There is a helpline number available'\n",
    "     ' at (123) 456-789')\n",
    "\n",
    "def extract_phone_number(nlp_doc):\n",
    "     pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
    "                {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
    "                {'ORTH': '-', 'OP': '?'},\n",
    "                {'SHAPE': 'ddd'}]\n",
    "     matcher.add('PHONE_NUMBER', None, pattern)\n",
    "     matches = matcher(nlp_doc)\n",
    "     for match_id, start, end in matches:\n",
    "         span = nlp_doc[start:end]\n",
    "         return span.text\n",
    "\n",
    "conference_org_doc = nlp(conference_org_text)\n",
    "extract_phone_number(conference_org_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILIOU way of annotating - ( GOLD PARSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "text = u\"This is Google Inc. \"\n",
    "entities = [(8, 19, 'ORG')]\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp.make_doc(text)\n",
    "gold = spacy.gold.GoldParse(doc, entities=entities)\n",
    "gold.orig_annot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using EntityRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy.gold import GoldParse \n",
    "# from spacy.language import EntityRecognizer \n",
    "from spacy.pipeline import EntityRecognizer\n",
    "\n",
    "nlp = spacy.load('en', entity = False, parser = False) \n",
    "doc_list = [] \n",
    "doc = nlp('Llamas make great pets.') \n",
    "doc_list.append(doc) \n",
    "gold_list = [] \n",
    "gold_list.append(GoldParse(doc, [u'ANIMAL', u'O', u'O', u'O'])) \n",
    "\n",
    "ner = EntityRecognizer(nlp.vocab, entity_types = ['ANIMAL']) \n",
    "ner.update(doc_list, gold_list) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By passing exact indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses with gold {'ner': 4.947793365689627}\n",
      "Losses with gold {'ner': 7.457957863807753}\n",
      "Losses with gold {'ner': 5.987422680586395}\n",
      "Losses with gold {'ner': 8.068778574466705}\n",
      "Losses with gold {'ner': 5.454485620803871}\n",
      "Losses with gold {'ner': 8.001810789108276}\n",
      "Losses with gold {'ner': 7.322129189968109}\n",
      "Losses with gold {'ner': 5.561052802109079}\n",
      "Losses with gold {'ner': 8.695716381072998}\n",
      "Losses with gold {'ner': 5.7760476442449065}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {'entities': [(20, 28, 'GADGET')]})\n",
    "    #Lots of other things\n",
    "]\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm',disable=['parser','tagger'])\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "LABEL=\"GADGET\"\n",
    "ner=nlp.get_pipe('ner')\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "\n",
    "for iteration in range(10):\n",
    "\n",
    "#     random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    for text, annotations in TRAINING_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        entity_offsets = annotations[\"entities\"]\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "        nlp.update([doc], [gold], drop=0.5, sgd=optimizer, losses=losses)\n",
    "        print('Losses with gold', losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BILOU file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entries of BILUO file:\n",
      "       word label\n",
      "5       on     O\n",
      "6     self     O\n",
      "7        -     O\n",
      "8  driving     O\n",
      "9     cars     O\n",
      "============================\n",
      "\n",
      "[Existing Entities]:\n",
      " ['B-ORG', 'B-DATE', 'B-PERSON', 'B-GPE', 'B-MONEY', 'B-CARDINAL', 'B-NORP', 'B-PERCENT', 'B-WORK_OF_ART', 'B-LOC', 'B-TIME', 'B-QUANTITY', 'B-FAC', 'B-EVENT', 'B-ORDINAL', 'B-PRODUCT', 'B-LAW', 'B-LANGUAGE', 'I-ORG', 'I-DATE', 'I-PERSON', 'I-GPE', 'I-MONEY', 'I-CARDINAL', 'I-NORP', 'I-PERCENT', 'I-WORK_OF_ART', 'I-LOC', 'I-TIME', 'I-QUANTITY', 'I-FAC', 'I-EVENT', 'I-ORDINAL', 'I-PRODUCT', 'I-LAW', 'I-LANGUAGE', 'L-ORG', 'L-DATE', 'L-PERSON', 'L-GPE', 'L-MONEY', 'L-CARDINAL', 'L-NORP', 'L-PERCENT', 'L-WORK_OF_ART', 'L-LOC', 'L-TIME', 'L-QUANTITY', 'L-FAC', 'L-EVENT', 'L-ORDINAL', 'L-PRODUCT', 'L-LAW', 'L-LANGUAGE', 'U-ORG', 'U-DATE', 'U-PERSON', 'U-GPE', 'U-MONEY', 'U-CARDINAL', 'U-NORP', 'U-PERCENT', 'U-WORK_OF_ART', 'U-LOC', 'U-TIME', 'U-QUANTITY', 'U-FAC', 'U-EVENT', 'U-ORDINAL', 'U-PRODUCT', 'U-LAW', 'U-LANGUAGE', 'O']\n",
      "\n",
      "\n",
      "[New Entities]: \n",
      " ['U-DATED', 'B-DATED', 'I-DATED', 'L-DATED']\n",
      "\n",
      "\n",
      "\n",
      "[OtherPipes] = ['tagger', 'parser'] will be disabled\n",
      "Losses {'ner': 12.865193346748185}\n",
      "Losses {'ner': 7.6070811310500455}\n",
      "Losses {'ner': 3.9828577556662554}\n",
      "Losses {'ner': 2.0058255619340075}\n",
      "Losses {'ner': 0.03442588766420385}\n",
      "Losses {'ner': 0.00016981573213899033}\n",
      "Losses {'ner': 1.0510333725737448e-05}\n",
      "Losses {'ner': 2.2181669623872944e-06}\n",
      "Losses {'ner': 1.0087090327899918e-06}\n",
      "Losses {'ner': 5.823040732181247e-07}\n",
      "Losses {'ner': 3.594350321446287e-07}\n",
      "Losses {'ner': 2.1489805717487328e-07}\n",
      "Losses {'ner': 1.3213264723088773e-07}\n",
      "Losses {'ner': 8.233809374934084e-08}\n",
      "Losses {'ner': 5.156538690828329e-08}\n",
      "Losses {'ner': 3.3975076737441643e-08}\n",
      "Losses {'ner': 2.393272415730718e-08}\n",
      "Losses {'ner': 1.7525404042592343e-08}\n",
      "Losses {'ner': 1.3385401836857268e-08}\n",
      "Losses {'ner': 1.0456359057358857e-08}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "words = []\n",
    "labels = []\n",
    "\n",
    "for token in doc:\n",
    "\twords.append(token.text)\n",
    "\tlabels.append('O') # As most of token will be non-entity (OUT). Replace this later with actual entity a/c the scheme.\n",
    "\n",
    "df = pd.DataFrame({'word': words, 'label': labels})\n",
    "df.to_csv('ner-token-per-line.biluo', index=False) # biluo in extension to indicate the type of encoding, it is ok to keep csv\n",
    "\n",
    "print(\"Sample entries of BILUO file:\\n\",df[5:10])\n",
    "print(\"============================\")\n",
    "\n",
    "#Upload the BILIOU format file\n",
    "dpath = 'ner-token-per-line.biluo'\n",
    "\n",
    "df = pd.read_csv(dpath, sep=',')\n",
    "words  = df.word.values\n",
    "ents = df.label.values\n",
    "text = ' '.join(words)\n",
    "\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "g = GoldParse(doc, entities=ents)\n",
    "X = [doc]\n",
    "Y = [g]\n",
    "\n",
    "add_ents = ['DATED'] # The new entity\n",
    "# Piplines in core pretrained model are tagger, parser, ner. Create new if blank model is to be trained using `spacy.blank('en')` else get the existing one.\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\") # \"architecture\": \"ensemble\" simple_cnn ensemble, bow # https://spacy.io/api/annotation\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "prev_ents = ner.move_names # All the existing entities recognised by the model\n",
    "print('\\n[Existing Entities]:\\n', ner.move_names)\n",
    "for ent in add_ents:\n",
    "    ner.add_label(ent)\n",
    "    \n",
    "new_ents = ner.move_names\n",
    "# print('\\n[All Entities] = ', ner.move_names)\n",
    "print('\\n\\n[New Entities]: \\n', list(set(new_ents) - set(prev_ents)))\n",
    "print('\\n\\n')\n",
    "## Training\n",
    "model = None # Since we are training a fresh model not a saved model\n",
    "n_iter = 20\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "print(f'[OtherPipes] = {other_pipes} will be disabled')\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # only train ner\n",
    "    # optimizer = nlp.begin_training()\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        nlp.update(X, Y,  sgd=optimizer, drop=0.0, losses=losses)\n",
    "        # nlp.entity.update(d, g)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link for other ways of GoldParse : \n",
    "https://github.com/prem2017/new-entity-labelling/blob/master/new_entity_labelling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "def evaluate(model, examples):\n",
    "  scorer = Scorer()\n",
    "  for input_, annot in examples:\n",
    "    #print(input_)\n",
    "    doc_gold_text = model.make_doc(input_)\n",
    "    gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
    "    pred_value = model(input_)\n",
    "    scorer.score(pred_value, gold)\n",
    "  return scorer.scores\n",
    "\n",
    "test_result = evaluate(new_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
